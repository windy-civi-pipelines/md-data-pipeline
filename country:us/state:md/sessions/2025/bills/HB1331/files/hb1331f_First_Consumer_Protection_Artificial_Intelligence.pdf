HOUSE BILL 1331
I3, S1 5lr0741
CF 5lr2992
By: Delegate Qi
Introduced and read first time: February 7, 2025
Assigned to: Economic Matters
A BILL ENTITLED
1 AN ACT concerning
2 Consumer Protection – Artificial Intelligence
3 FOR the purpose of regulating the manner in which a developer or deployer of artificial
4 intelligence must protect consumers from certain risks; requiring a developer that
5 offers to sell a certain artificial intelligence system to provide certain information
6 and make certain disclosures; requiring a deployer to implement a certain risk
7 management policy and take certain precautions to protect consumers from certain
8 risks; requiring a deployer to complete a certain impact assessment and make
9 certain disclosures; authorizing a deployer to decline certain appeals; providing
10 exceptions to certain requirements for a deployer; establishing a certain rebuttable
11 presumption; making a certain violation an unfair, abusive, or deceptive trade
12 practice that is subject to enforcement and penalties under the Maryland Consumer
13 Protection Act; establishing requirements for certain agreements regarding voice
14 and likeness clones; and generally relating to artificial intelligence.
15 BY repealing and reenacting, with amendments,
16 Article – Commercial Law
17 Section 13–301(14)(xlii)
18 Annotated Code of Maryland
19 (2013 Replacement Volume and 2024 Supplement)
20 BY repealing and reenacting, without amendments,
21 Article – Commercial Law
22 Section 13–301(14)(xliii)
23 Annotated Code of Maryland
24 (2013 Replacement Volume and 2024 Supplement)
25 BY adding to
26 Article – Commercial Law
27 Section 13–301(14)(xliv); 14–5001 through 14–5008 to be under the new subtitle
28 “Subtitle 50. Developers and Deployers of Artificial Intelligence Systems”; and
EXPLANATION: CAPITALS INDICATE MATTER ADDED TO EXISTING LAW.
[Brackets] indicate matter deleted from exist in g l aw . *hb1331*

2 HOUSE BILL 1331
1 14–5101 and 14–5102 to be under the new subtitle “Subtitle 51. Agreements
2 Concerning Voice and Likeness Clones”
3 Annotated Code of Maryland
4 (2013 Replacement Volume and 2024 Supplement)
5 BY repealing and reenacting, without amendments,
6 Article – State Finance and Procurement
7 Section 3.5–801(a) and (c)
8 Annotated Code of Maryland
9 (2021 Replacement Volume and 2024 Supplement)
10 SECTION 1. BE IT ENACTED BY THE GENERAL ASSEMBLY OF MARYLAND,
11 That the Laws of Maryland read as follows:
12 Article – Commercial Law
13 13–301.
14 Unfair, abusive, or deceptive trade practices include any:
15 (14) Violation of a provision of:
16 (xlii) Section 12–6C–09.1 of the Health Occupations Article; [or]
17 (xliii) Title 14, Subtitle 48 of this article; or
18 (XLIV) TITLE 14, SUBTITLE 50 OF THIS ARTICLE; OR
19 SUBTITLE 50. DEVELOPERS AND DEPLOYERS OF ARTIFICIAL INTELLIGENCE
20 SYSTEMS.
21 14–5001.
22 (A) IN THIS SUBTITLE THE FOLLOWING WORDS HAVE THE MEANINGS
23 INDICATED.
24 (B) (1) “ALGORITHMIC DISCRIMINATION” MEANS DIFFERENTIAL
25 TREATMENT AS A RESULT OF THE USE OF ARTIFICIAL INTELLIGENCE THAT
26 NEGATIVELY IMPACTS A PERSON BASED ON THE PERSON’S ACTUAL OR PERCEIVED
27 AGE, COLOR, DISABILITY, ETHNICITY, GENETIC INFORMATION, PROFICIENCY IN THE
28 ENGLISH LANGUAGE, NATIONAL ORIGIN, RACE, RELIGION, REPRODUCTIVE HEALTH,
29 SEX, VETERAN STATUS, OR OTHER PROTECTED CLASS.
30 (2) “ALGORITHMIC DISCRIMINATION” DOES NOT INCLUDE:

HOUSE BILL 1331 3
1 (I) THE OFFER, LICENSE, OR USE OF A HIGH–RISK ARTIFICIAL
2 INTELLIGENCE SYSTEM BY A DEVELOPER OR DEPLOYER FOR:
3 1. TESTING TO IDENTIFY, MITIGATE, OR PREVENT
4 DISCRIMINATION OR ENSURE COMPLIANCE WITH A STATE OR FEDERAL LAW; OR
5 2. INCREASING DIVERSITY WITHIN AN APPLICANT,
6 CUSTOMER, OR PARTICIPANT POOL; OR
7 (II) AN ACTION THAT BENEFITS AN ESTABLISHMENT THAT:
8 1. IS SUBJECT TO 42 U.S.C. § 2000A(E); AND
9 2. IS NOT IN FACT OPEN TO THE PUBLIC.
10 (C) “ARTIFICIAL INTELLIGENCE” HAS THE MEANING STATED IN § 3.5–801
11 OF THE STATE FINANCE AND PROCUREMENT ARTICLE.
12 (D) “DATASET CARD” MEANS A FILE THAT:
13 (1) IS USED TO INFORM USERS ABOUT HOW TO RESPONSIBLY USE THE
14 DATA IN A DATASET; AND
15 (2) CONTAINS INFORMATION ABOUT POTENTIAL BIASES OF THE
16 DATA.
17 (E) “DECISION THAT PRODUCES LEGAL OR SIMILARLY SIGNIFICANT
18 EFFECTS CONCERNING THE CONSUMER” HAS THE MEANING STATED IN § 14–4701
19 OF THIS TITLE.
20 (F) “HIGH–RISK ARTIFICIAL INTELLIGENCE SYSTEM” MEANS ANY
21 ARTIFICIAL INTELLIGENCE SYSTEM THAT, WHEN DEPLOYED, MAKES, OR IS A
22 SUBSTANTIAL FACTOR IN MAKING, A DECISION THAT PRODUCES LEGAL OR
23 SIMILARLY SIGNIFICANT EFFECTS CONCERNING THE CONSUMER.
24 (G) “MODEL CARD” MEANS A FILE THAT ACCOMPANIES THE MODEL AND
25 PROVIDES INFORMATION ABOUT DISCOVERABILITY, REPRODUCIBILITY, AND
26 SHARING.
27 14–5002.
28 (A) A DEVELOPER SHALL TAKE REASONABLE PRECAUTIONS TO PROTECT
29 CONSUMERS FROM KNOWN OR REASONABLY FORESEEABLE RISKS OF ALGORITHMIC

4 HOUSE BILL 1331
1 DISCRIMINATION FROM THE INTENDED USES OF A HIGH–RISK ARTIFICIAL
2 INTELLIGENCE SYSTEM PRODUCED BY THE DEVELOPER.
3 (B) A DEVELOPER THAT OFFERS TO SELL A HIGH–RISK ARTIFICIAL
4 INTELLIGENCE SYSTEM SHALL:
5 (1) PROVIDE TO A PURCHASER OF THE HIGH–RISK ARTIFICIAL
6 INTELLIGENCE SYSTEM THE STANDARDIZED DISCLOSURE DOCUMENTATION
7 PERTAINING TO THE USES OF THE HIGH–RISK ARTIFICIAL INTELLIGENCE SYSTEM
8 AS DESCRIBED IN SUBSECTION (C) OF THIS SECTION;
9 (2) PROVIDE TO A DEPLOYER INFORMATION NECESSARY FOR THE
10 DEPLOYER TO COMPLETE AN IMPACT ASSESSMENT UNDER § 14–5004 OF THIS
11 SUBTITLE; AND
12 (3) PUBLISH, ON THE DEVELOPER’S WEBSITE OR IN A PUBLIC USE
13 CASE INVENTORY, STANDARDIZED INFORMATION PERTAINING TO EACH HIGH–RISK
14 ARTIFICIAL INTELLIGENCE SYSTEM THAT THE DEVELOPER OFFERS FOR PURCHASE.
15 (C) THE STANDARDIZED DISCLOSURE DOCUMENTATION REQUIRED UNDER
16 SUBSECTION (B)(1) OF THIS SECTION SHALL INCLUDE:
17 (1) THE PURPOSE OF THE HIGH–RISK ARTIFICIAL INTELLIGENCE
18 SYSTEM;
19 (2) THE INTENDED USES, BENEFITS, AND OUTPUTS OF THE
20 HIGH–RISK ARTIFICIAL INTELLIGENCE SYSTEM;
21 (3) A DESCRIPTION OF THE KNOWN AND REASONABLY FORESEEABLE:
22 (I) USES AND MISUSES OF THE HIGH–RISK ARTIFICIAL
23 INTELLIGENCE SYSTEM, AND THE RISKS OF ALGORITHMIC DISCRIMINATION
24 ARISING FROM THEM; AND
25 (II) LIMITATIONS OF THE HIGH–RISK ARTIFICIAL
26 INTELLIGENCE SYSTEM;
27 (4) A DESCRIPTION OF THE STEPS TAKEN BY THE DEVELOPER PRIOR
28 TO OFFERING A HIGH–RISK ARTIFICIAL INTELLIGENCE SYSTEM FOR PURCHASE TO:
29 (I) EVALUATE THE PERFORMANCE OF THE HIGH–RISK
30 ARTIFICIAL INTELLIGENCE SYSTEM; AND

HOUSE BILL 1331 5
1 (II) MITIGATE KNOWN OR REASONABLY FORESEEABLE
2 ALGORITHMIC DISCRIMINATION ARISING FROM THE USE OF THE HIGH–RISK
3 ARTIFICIAL INTELLIGENCE SYSTEM;
4 (5) A SUMMARY OF THE DATA USED TO TRAIN THE HIGH–RISK
5 ARTIFICIAL INTELLIGENCE SYSTEM;
6 (6) A DESCRIPTION OF THE DATA GOVERNANCE MEASURES THAT
7 APPLIED TO THE DATA USED TO TRAIN THE HIGH–RISK ARTIFICIAL INTELLIGENCE
8 SYSTEM;
9 (7) THE MEASURES TAKEN TO EXAMINE:
10 (I) THE SUITABILITY OF DATA SOURCES; AND
11 (II) POSSIBLE BIASES AND APPROPRIATE MITIGATION;
12 (8) BEST PRACTICES FOR THE INTENDED USE, PREVENTION OF
13 MISUSE, AND MONITORING OF A HIGH–RISK ARTIFICIAL INTELLIGENCE SYSTEM;
14 (9) ANY INFORMATION NECESSARY FOR THE PURCHASER TO COMPLY
15 WITH THE REQUIREMENTS UNDER § 14–5003 OF THIS SUBTITLE; AND
16 (10) ANY INFORMATION REASONABLY NECESSARY TO ALLOW THE
17 PURCHASER TO UNDERSTAND THE OUTPUTS OF AND MONITOR THE PERFORMANCE
18 OF THE HIGH–RISK ARTIFICIAL INTELLIGENCE SYSTEM.
19 (D) THE INFORMATION REQUIRED UNDER SUBSECTION (C)(2) OF THIS
20 SECTION SHALL, TO THE EXTENT FEASIBLE, BE PROVIDED IN THE FORM OF MODEL
21 CARDS, DATASET CARDS, OR PREEXISTING IMPACT ASSESSMENTS.
22 (E) THE DOCUMENTATION REQUIRED TO BE PUBLISHED UNDER
23 SUBSECTION (C)(3) OF THIS SECTION SHALL:
24 (1) SUMMARIZE THE HIGH–RISK ARTIFICIAL INTELLIGENCE SYSTEMS
25 THAT THE DEVELOPER OFFERS FOR SALE;
26 (2) SUMMARIZE THE MEASURES TAKEN BY THE DEVELOPER TO
27 MANAGE KNOWN AND REASONABLY FORESEEABLE RISKS OF ALGORITHMIC
28 DISCRIMINATION THAT THE DEVELOPER OFFERS FOR SALE; AND
29 (3) BE UPDATED:

6 HOUSE BILL 1331
1 (I) AS NECESSARY TO ENSURE THAT THE DOCUMENTATION
2 REMAINS ACCURATE; AND
3 (II) NOT LATER THAN 90 DAYS AFTER ANY CHANGE IS MADE TO
4 A HIGH–RISK ARTIFICIAL INTELLIGENCE SYSTEM THAT WOULD NECESSITATE
5 UPDATING THE RELEVANT DOCUMENTATION UNDER SUBSECTION (C)(3) OF THIS
6 SECTION.
7 (F) A DEVELOPER THAT LEARNS THROUGH ONGOING INTERNAL TESTING,
8 OR THROUGH A CREDIBLE REPORT, THAT A HIGH–RISK ARTIFICIAL INTELLIGENCE
9 SYSTEM OFFERED FOR SALE BY THE DEVELOPER HAS CAUSED, OR IS LIKELY TO
10 CAUSE, ALGORITHMIC DISCRIMINATION SHALL DISCLOSE THE POTENTIAL FOR
11 ALGORITHMIC DISCRIMINATION TO:
12 (1) THE ATTORNEY GENERAL IN A FORM AND MANNER DETERMINED
13 BY THE ATTORNEY GENERAL; AND
14 (2) ALL PURCHASERS OF THE HIGH–RISK ARTIFICIAL INTELLIGENCE
15 SYSTEM.
16 (G) NOTHING IN THIS SECTION MAY BE CONSTRUED TO REQUIRE A
17 DEVELOPER TO DISCLOSE:
18 (1) A TRADE SECRET;
19 (2) INFORMATION PROTECTED FROM DISCLOSURE BY STATE OR
20 FEDERAL LAW; OR
21 (3) INFORMATION THAT WOULD OTHERWISE CREATE A SECURITY
22 RISK TO THE DEVELOPER.
23 (H) THE ATTORNEY GENERAL MAY REQUIRE A DEVELOPER TO DISCLOSE TO
24 THE ATTORNEY GENERAL THE INFORMATION REQUIRED UNDER SUBSECTION (C) OF
25 THIS SECTION.
26 14–5003.
27 (A) A DEPLOYER SHALL TAKE REASONABLE PRECAUTIONS TO PROTECT
28 CONSUMERS FROM KNOWN OR REASONABLY FORESEEABLE RISKS OF ALGORITHMIC
29 DISCRIMINATION POSED BY THE INTENDED USES OF A HIGH–RISK ARTIFICIAL
30 INTELLIGENCE SYSTEM DEPLOYED BY THE DEPLOYER.

HOUSE BILL 1331 7
1 (B) A DEPLOYER SHALL IMPLEMENT A RISK MANAGEMENT POLICY TO
2 GOVERN THE DEPLOYMENT OF A HIGH–RISK ARTIFICIAL INTELLIGENCE SYSTEM.
3 (C) A RISK MANAGEMENT POLICY UNDER SUBSECTION (B) OF THIS SECTION
4 SHALL:
5 (1) IDENTIFY, DOCUMENT, AND MITIGATE KNOWN AND REASONABLY
6 FORESEEABLE RISKS OF ALGORITHMIC DISCRIMINATION;
7 (2) SPECIFY AND INCORPORATE PRINCIPLES, PROCESSES, AND
8 PERSONNEL USED TO COMPLY WITH ITEM (1) OF THIS SUBSECTION;
9 (3) BE PLANNED TO SPAN, AND BE IMPLEMENTED FOR, THE ENTIRE
10 PERIOD DURING WHICH THE HIGH–RISK ARTIFICIAL INTELLIGENCE SYSTEM IS TO
11 BE DEPLOYED; AND
12 (4) BE REGULARLY AND SYSTEMATICALLY REVIEWED AND UPDATED
13 THROUGHOUT THE PERIOD DURING WHICH THE HIGH–RISK ARTIFICIAL
14 INTELLIGENCE SYSTEM IS DEPLOYED.
15 (D) A RISK MANAGEMENT POLICY UNDER SUBSECTION (B) OF THIS SECTION
16 MAY APPLY TO MORE THAN ONE HIGH–RISK ARTIFICIAL INTELLIGENCE SYSTEM.
17 (E) IN CREATING A RISK MANAGEMENT POLICY UNDER SUBSECTION (C) OF
18 THIS SECTION, A DEPLOYER SHALL CONSIDER AND REASONABLY ADDRESS:
19 (1) (I) GUIDANCE SET FORTH IN THE MOST UP–TO–DATE VERSION
20 OF THE “ARTIFICIAL INTELLIGENCE RISK MANAGEMENT FRAMEWORK”
21 PUBLISHED BY THE NATIONAL INSTITUTE OF STANDARDS AND TECHNOLOGY;
22 (II) ISE/IEC 42001 OF THE INTERNATIONAL ORGANIZATION
23 FOR STANDARDIZATION;
24 (III) ANOTHER INTERNATIONALLY RECOGNIZED ARTIFICIAL
25 INTELLIGENCE RISK MANAGEMENT FRAMEWORK THAT CONTAINS REQUIREMENTS
26 AT LEAST AS STRINGENT AS THIS SUBTITLE; OR
27 (IV) ANOTHER RISK MANAGEMENT FRAMEWORK DESIGNATED
28 BY THE ATTORNEY GENERAL;
29 (2) THE SIZE AND COMPLEXITY OF THE DEPLOYER;

8 HOUSE BILL 1331
1 (3) THE NATURE, USES, AND SCOPE OF THE HIGH–RISK ARTIFICIAL
2 INTELLIGENCE SYSTEM; AND
3 (4) THE SENSITIVITY AND VOLUME OF DATA PROCESSED BY THE
4 HIGH–RISK ARTIFICIAL INTELLIGENCE SYSTEM.
5 14–5004.
6 (A) EXCEPT AS OTHERWISE PROVIDED IN THIS SUBTITLE, A DEPLOYER
7 SHALL:
8 (1) COMPLETE AN IMPACT ASSESSMENT OF ANY DEPLOYED
9 HIGH–RISK ARTIFICIAL INTELLIGENCE SYSTEM;
10 (2) RETAIN ALL IMPACT ASSESSMENTS AND RECORDS RELATING TO
11 AN IMPACT ASSESSMENT FOR AT LEAST 3 YEARS AFTER THE END OF THE
12 DEPLOYMENT OF THE HIGH–RISK ARTIFICIAL INTELLIGENCE SYSTEM;
13 (3) NOT LESS THAN ONCE EACH YEAR, ASSESS WHETHER A
14 HIGH–RISK ARTIFICIAL INTELLIGENCE SYSTEM DEPLOYED BY THE DEPLOYER IS
15 CAUSING ALGORITHMIC DISCRIMINATION;
16 (4) MAKE AVAILABLE, AT THE TIME OF DEPLOYMENT, A
17 STANDARDIZED DISCLOSURE THAT MEETS THE REQUIREMENTS OF § 14–5005 OF
18 THIS SUBTITLE;
19 (5) PROVIDE A CONSUMER SUBJECT TO A HIGH–RISK ARTIFICIAL
20 INTELLIGENCE SYSTEM DEPLOYED BY THE DEPLOYER AN OPPORTUNITY TO:
21 (I) CORRECT ANY DATA PROCESSED BY THE HIGH–RISK
22 ARTIFICIAL INTELLIGENCE SYSTEM IN MAKING A DECISION ABOUT THE CONSUMER;
23 AND
24 (II) APPEAL AN ADVERSE DECISION MADE USING A HIGH–RISK
25 ARTIFICIAL INTELLIGENCE SYSTEM, INCLUDING HUMAN REVIEW OF THE DECISION;
26 AND
27 (6) MAKE A CLEAR AND PROMINENTLY DISPLAYED STATEMENT
28 AVAILABLE ON THE DEPLOYER’S WEBSITE THAT:
29 (I) SUMMARIZES THE HIGH–RISK ARTIFICIAL INTELLIGENCE
30 SYSTEMS CURRENTLY DEPLOYED BY THE DEPLOYER;

HOUSE BILL 1331 9
1 (II) SUMMARIZES THE DEPLOYER’S PRACTICES FOR MANAGING
2 THE KNOWN OR REASONABLY FORESEEABLE RISKS OF ALGORITHMIC
3 DISCRIMINATION APPLICABLE TO DEPLOYED HIGH–RISK ARTIFICIAL
4 INTELLIGENCE SYSTEMS;
5 (III) DETAILS THE NATURE, SOURCE, AND EXTENT OF PERSONAL
6 INFORMATION COLLECTED OR USED BY THE DEPLOYER; AND
7 (IV) IS UPDATED PERIODICALLY.
8 (B) A DEPLOYER MAY DECLINE TO PROVIDE AN OPPORTUNITY FOR APPEAL
9 IF THE APPEAL WOULD BE AGAINST THE BEST INTERESTS OF THE CONSUMER,
10 INCLUDING SITUATIONS IN WHICH THE DELAY CAUSED BY THE APPEAL COULD POSE
11 A RISK TO THE SAFETY OF THE CONSUMER.
12 (C) A DEPLOYER NEED NOT PROVIDE A RISK MANAGEMENT POLICY, AN
13 IMPACT ASSESSMENT, OR THE DISCLOSURES REQUIRED UNDER SUBSECTION (A)(6)
14 OF THIS SECTION IF THE DEPLOYER:
15 (1) EMPLOYS FEWER THAN 50 FULL–TIME EQUIVALENT EMPLOYEES;
16 (2) TRAINS THE HIGH–RISK ARTIFICIAL INTELLIGENCE SYSTEM ON
17 DATA OTHER THAN DATA COLLECTED BY THE DEPLOYER;
18 (3) DOES NOT TRAIN THE HIGH–RISK ARTIFICIAL INTELLIGENCE
19 SYSTEM ON DATA THAT THE DEPLOYER COLLECTS;
20 (4) USES THE HIGH–RISK ARTIFICIAL INTELLIGENCE SYSTEM AS
21 INTENDED BY ITS DEVELOPER, AS DISCLOSED UNDER § 14–5002(C) OF THIS
22 SUBTITLE; AND
23 (5) MAKES AVAILABLE TO A CONSUMER AN IMPACT ASSESSMENT:
24 (I) COMPLETED BY AND PROVIDED TO THE DEPLOYER BY THE
25 DEVELOPER OF THE HIGH–RISK ARTIFICIAL INTELLIGENCE SYSTEM; AND
26 (II) THAT IS SUBSTANTIALLY SIMILAR TO AN IMPACT
27 ASSESSMENT OTHERWISE REQUIRED UNDER THIS SECTION.
28 14–5005.
29 A STANDARDIZED DISCLOSURE UNDER § 14–5004(A)(4) OF THIS SUBTITLE
30 SHALL:

10 HOUSE BILL 1331
1 (1) NOTIFY A CONSUMER SUBJECT TO THE HIGH–RISK ARTIFICIAL
2 INTELLIGENCE SYSTEM THAT A HIGH–RISK ARTIFICIAL INTELLIGENCE SYSTEM IS IN
3 USE;
4 (2) DISCLOSE THE PURPOSE OF THE HIGH–RISK ARTIFICIAL
5 INTELLIGENCE SYSTEM IN USE, INCLUDING THE NATURE OF A DECISION ABOUT A
6 CONSUMER THAT IS MADE BY THE HIGH–RISK ARTIFICIAL INTELLIGENCE SYSTEM;
7 (3) PROVIDE THE CONTACT INFORMATION OF THE DEPLOYER;
8 (4) PROVIDE A PLAIN LANGUAGE DESCRIPTION OF THE HIGH–RISK
9 ARTIFICIAL INTELLIGENCE SYSTEM;
10 (5) PROVIDE INSTRUCTIONS REGARDING HOW TO ACCESS THE
11 STATEMENT REQUIRED UNDER § 14–5004(C)(5) OF THIS SUBTITLE;
12 (6) DISCLOSE THE DECISION THAT THE HIGH–RISK ARTIFICIAL
13 INTELLIGENCE SYSTEM IS DEPLOYED TO MAKE OR CONTRIBUTE TO MAKING;
14 (7) DISCLOSE THE REASON FOR THE DECISION THAT THE HIGH–RISK
15 ARTIFICIAL INTELLIGENCE SYSTEM IS DEPLOYED TO MAKE OR CONTRIBUTE TO
16 MAKING;
17 (8) DISCLOSE THE DEGREE TO WHICH THE HIGH–RISK ARTIFICIAL
18 INTELLIGENCE SYSTEM IS INVOLVED IN MAKING THE DECISION DISCLOSED UNDER
19 ITEM (6) OF THIS SECTION;
20 (9) DISCLOSE THE DATA THAT THE HIGH–RISK ARTIFICIAL
21 INTELLIGENCE SYSTEM USES IN MAKING OR CONTRIBUTING TO MAKING THE
22 DECISION AND ITS SOURCE;
23 (10) BE PROVIDED DIRECTLY TO THE CONSUMER, OR IN A MANNER
24 REASONABLY CALCULATED TO BE ACCESSIBLE BY THE CONSUMER IF PROVIDING
25 THE DISCLOSURE DIRECTLY IS NOT POSSIBLE;
26 (11) BE IN PLAIN LANGUAGE;
27 (12) BE MADE AVAILABLE IN EACH LANGUAGE THAT THE DEPLOYER
28 REGULARLY USES IN CONTRACTS, DISCLAIMERS, SALE ANNOUNCEMENTS, AND
29 OTHER INFORMATION PROVIDED TO CONSUMERS; AND

HOUSE BILL 1331 11
1 (13) BE MADE AVAILABLE IN A FORMAT THAT IS ACCESSIBLE TO
2 CONSUMERS WITH DISABILITIES.
3 14–5006.
4 (A) AN IMPACT ASSESSMENT SHALL BE COMPLETED:
5 (1) AT LEAST ONCE EACH YEAR; AND
6 (2) WITHIN 90 DAYS AFTER AN INTENTIONAL AND SUBSTANTIAL
7 MODIFICATION OF THE HIGH–RISK ARTIFICIAL INTELLIGENCE SYSTEM.
8 (B) AN IMPACT ASSESSMENT UNDER THIS SECTION SHALL INCLUDE:
9 (1) THE PURPOSE, INTENDED USE CASES, AND BENEFITS OF THE
10 HIGH–RISK ARTIFICIAL INTELLIGENCE SYSTEM;
11 (2) THE CONTEXT IN WHICH THE HIGH–RISK ARTIFICIAL
12 INTELLIGENCE SYSTEM WAS DEPLOYED;
13 (3) AN ANALYSIS OF KNOWN OR REASONABLY FORESEEABLE RISKS
14 OF ALGORITHMIC DISCRIMINATION POSED BY THE HIGH–RISK ARTIFICIAL
15 INTELLIGENCE SYSTEM;
16 (4) A DESCRIPTION OF STEPS TAKEN TO MITIGATE KNOWN OR
17 REASONABLY FORESEEABLE RISKS OF ALGORITHMIC DISCRIMINATION POSED BY
18 THE HIGH–RISK ARTIFICIAL INTELLIGENCE SYSTEM;
19 (5) A DESCRIPTION OF THE INPUTS AND OUTPUTS OF THE HIGH–RISK
20 ARTIFICIAL INTELLIGENCE SYSTEM;
21 (6) A DESCRIPTION OF ANY DATA USED BY A DEVELOPER TO
22 CUSTOMIZE THE HIGH–RISK ARTIFICIAL INTELLIGENCE SYSTEM;
23 (7) A DESCRIPTION OF ANY METRICS USED TO EVALUATE THE
24 PERFORMANCE OF THE HIGH–RISK ARTIFICIAL INTELLIGENCE SYSTEM;
25 (8) A DESCRIPTION OF KNOWN LIMITATIONS OF THE HIGH–RISK
26 ARTIFICIAL INTELLIGENCE SYSTEM;
27 (9) A DESCRIPTION OF TRANSPARENCY MEASURES IMPLEMENTED
28 REGARDING THE HIGH–RISK ARTIFICIAL INTELLIGENCE SYSTEM, INCLUDING

12 HOUSE BILL 1331
1 MEASURES TAKEN TO DISCLOSE THAT THE SYSTEM IS IN USE TO A CONSUMER WHEN
2 THE SYSTEM IS IN USE;
3 (10) A DESCRIPTION OF THE DEPLOYER’S OVERSIGHT PROCESSES,
4 INCLUDING:
5 (I) USER SAFEGUARDS PROVIDED; AND
6 (II) THE DEPLOYER’S PROCESS FOR ADDRESSING ISSUES
7 ARISING FROM THE DEPLOYMENT OF THE HIGH–RISK ARTIFICIAL INTELLIGENCE
8 SYSTEM; AND
9 (11) FOR ANY IMPACT ASSESSMENT FOLLOWING THE INITIAL IMPACT
10 ASSESSMENT OF A HIGH–RISK ARTIFICIAL INTELLIGENCE SYSTEM, A DESCRIPTION
11 OF THE EXTENT TO WHICH THE USE OF THE HIGH–RISK ARTIFICIAL INTELLIGENCE
12 SYSTEM WAS CONSISTENT WITH THE DEVELOPER’S INTENDED USES.
13 (C) AN IMPACT ASSESSMENT UNDER SUBSECTION (B) OF THIS SECTION
14 MAY:
15 (1) ADDRESS MULTIPLE SUBSTANTIALLY SIMILAR HIGH–RISK
16 ARTIFICIAL INTELLIGENCE SYSTEMS; AND
17 (2) HAVE BEEN CREATED IN ORDER TO COMPLY WITH ANOTHER
18 REQUIREMENT OTHER THAN THIS SUBTITLE IF THE IMPACT ASSESSMENT SATISFIES
19 THE REQUIREMENTS OF THIS SUBTITLE.
20 14–5007.
21 (A) THERE IS A REBUTTABLE PRESUMPTION THAT A DEVELOPER TOOK
22 REASONABLE PRECAUTIONS UNDER § 14–5002(A) OF THIS SUBTITLE IF THE
23 DEVELOPER COMPLIED WITH:
24 (1) § 14–5002 OF THIS SUBTITLE; AND
25 (2) REGULATIONS ADOPTED BY THE ATTORNEY GENERAL.
26 (B) THERE IS A REBUTTABLE PRESUMPTION THAT A DEPLOYER TOOK
27 REASONABLE PRECAUTIONS UNDER § 14–5003(A) OF THIS SUBTITLE IF THE
28 DEPLOYER COMPLIED WITH:
29 (1) § 14–5003 OF THIS SUBTITLE; AND

HOUSE BILL 1331 13
1 (2) REGULATIONS ADOPTED BY THE ATTORNEY GENERAL.
2 (C) (1) THE ATTORNEY GENERAL MAY, IN A FORM AND MANNER
3 PRESCRIBED BY THE ATTORNEY GENERAL, REQUIRE A DEVELOPER OR DEPLOYER
4 TO PROVIDE DISCLOSURES OTHERWISE REQUIRED UNDER THIS SUBTITLE FOR
5 PURPOSES OF EVALUATING THE DISCLOSURE’S COMPLIANCE WITH THIS SUBTITLE.
6 (2) A DEVELOPER OR DEPLOYER MAY DECLINE TO PROVIDE THE
7 ATTORNEY GENERAL WITH INFORMATION THAT WOULD REQUIRE THE DISCLOSURE
8 OF TRADE SECRETS OR INFORMATION OTHERWISE PROTECTED BY STATE OR
9 FEDERAL LAW.
10 (3) INFORMATION PROVIDED TO THE ATTORNEY GENERAL UNDER
11 THIS SUBSECTION IS NOT SUBJECT TO DISCLOSURE UNDER THE MARYLAND PUBLIC
12 INFORMATION ACT.
13 (4) DISCLOSURE OF INFORMATION UNDER THIS SUBSECTION THAT IS
14 SUBJECT TO ATTORNEY–CLIENT PRIVILEGE OR WORK–PRODUCT PROTECTION DOES
15 NOT WAIVE THAT PRIVILEGE OR PROTECTION.
16 14–5008.
17 (A) EXCEPT AS PROVIDED IN SUBSECTION (B) OF THIS SECTION, A
18 VIOLATION OF THIS SUBTITLE IS:
19 (1) AN UNFAIR, ABUSIVE, OR DECEPTIVE TRADE PRACTICE WITHIN
20 THE MEANING OF TITLE 13 OF THIS ARTICLE; AND
21 (2) SUBJECT TO THE ENFORCEMENT AND PENALTY PROVISIONS
22 CONTAINED IN TITLE 13 OF THIS ARTICLE, EXCEPT FOR § 13–408 OF THIS ARTICLE.
23 (B) THIS SECTION DOES NOT PREVENT A CONSUMER FROM PURSUING ANY
24 OTHER REMEDY PROVIDED BY LAW.
25 SUBTITLE 51. AGREEMENTS CONCERNING VOICE AND LIKENESS CLONES.
26 14–5101.
27 (A) IN THIS SUBTITLE THE FOLLOWING WORDS HAVE THE MEANINGS
28 INDICATED.
29 (B) “ARTIFICIAL INTELLIGENCE” HAS THE MEANING STATED IN § 3.5–801
30 OF THE STATE FINANCE AND PROCUREMENT ARTICLE.

14 HOUSE BILL 1331
1 (C) “DIGITAL COPY” MEANS A NEWLY CREATED, ELECTRONIC
2 REPRESENTATION OF THE IDENTITY OF AN ACTUAL INDIVIDUAL CREATED USING A
3 COMPUTER, AN ALGORITHM, SOFTWARE, A TOOL, ARTIFICIAL INTELLIGENCE, OR
4 ANY OTHER TECHNOLOGY THAT IS:
5 (1) FIXED IN A SOUND RECORDING OR AUDIOVISUAL WORK IN WHICH
6 THE INDIVIDUAL DID NOT ACTUALLY PERFORM OR APPEAR; AND
7 (2) SO REALISTIC THAT A REASONABLE PERSON WOULD BELIEVE THE
8 DIGITAL COPY IS A PERFORMANCE BY THE INDIVIDUAL BEING PORTRAYED AND NOT
9 ANOTHER INDIVIDUAL.
10 14–5102.
11 AN AGREEMENT TO PERFORM PERSONAL OR PROFESSIONAL SERVICES IS
12 UNENFORCEABLE IF:
13 (1) THE AGREEMENT ALLOWS FOR THE CREATION OR USE OF A
14 DIGITAL COPY OF A PERSON’S VOICE OR IMAGE INSTEAD OF WORK PERFORMED LIVE
15 BY THE PERSON;
16 (2) THE AGREEMENT DOES NOT INCLUDE:
17 (I) A DESCRIPTION OF THE INTENDED USES OF THE DIGITAL
18 COPY THAT IS EASILY UNDERSTANDABLE; AND
19 (II) THE LICENSING TERMS GOVERNING THE USE OF THE
20 DIGITAL COPY; AND
21 (3) THE PERSON WAS NOT REPRESENTED BY:
22 (I) A LAWYER WHO NEGOTIATED ON BEHALF OF THE PERSON
23 LICENSING THE RIGHTS TO A DIGITAL COPY; OR
24 (II) A LABOR UNION REPRESENTING WORKERS WHO PERFORM
25 SIMILAR WORK WITH A COLLECTIVE BARGAINING AGREEMENT THAT EXPRESSLY
26 COVERS THE USE OF DIGITAL COPIES.
27 Article – State Finance and Procurement
28 3.5–801.
29 (a) In this subtitle the following words have the meanings indicated.

HOUSE BILL 1331 15
1 (c) “Artificial intelligence” means a machine–based system that:
2 (1) can, for a given set of human–defined objectives, make predictions,
3 recommendations, or decisions influencing real or virtual environments;
4 (2) uses machine and human–based inputs to perceive real and virtual
5 environments and abstracts those perceptions into models through analysis in an
6 automated manner; and
7 (3) uses model inference to formulate options for information or action.
8 SECTION 2. AND BE IT FURTHER ENACTED, That this Act shall take effect
9 October 1, 2025.

[DELETED: :CA      5   C A C F i o a m c d r a d t i a c r m r r a d t c a c i a a m c d a a d t d c a p e p m a c v a u a o d t p P e r a B  A S  A  ( B  A S  A  ( B  A S 1 t 1 t u t n s “]
[DELETED:  H  1 C  A  ( B  A S  A  (  S T A 1  U   ( V    ( S o]    ( T   ) T1,S50;VI  U  E 5.DDA  E E R N.() N T S T F W H T M() ( “AL DN M D A A R O T U O A I TNSE R Y Y N ,,,,,,,E N E N HX S ,. ( “A”:N ]
[DELETED: H 3  () T,,HR E HR   1 T, T IY ME O P TW ;   2 I D W A ATR L ;  () A:IN   1 I42U.§20();    AE    2 I.() “A”§3RE    T I R RFPA.() “D”:AD  ( T  (  I A P B O T() “E T P L O S SR   §1() “H–IHR A I SM M A I S TT W DD MS O I A, F I MG A D T P L O() “MOD M F A I A DY RY A()  ]
[DELETED:  H  F T I U O A HHR A() A D T O T S A HHR A ( P T A P O T HHR A S T S D DHR C ); (  T A D I N F T T C A I A U   O T1E  ( P,H O DRS W O I P UY HR() H B )): ( T P O T HHR A I ( T,, I US BS A O O THR; ( A:  () US A M O T HHR A SM A T R O A DM   () LII O T HHR A ( HR:  () EV T P O T HHRM ]
[DELETED: H 5  () II K O R F D A F T U O T HHR ( AS O T D U T T T HHR ( D O T D G M THR ( T:  () T;HS   () P;IO ( B P F T I UE P OE HR–; (    E 13; (  I R N T A THR.() T()H I R U S C O TL E S S ,.() H D R T B P U C )): ( SHR (  T M T B T D T K A R F R O AE  ( B:]
[DELETED:  H   () S N T E T T DE   () N9IO   HHR A I S T W N T R D U S C O)() A GT HR S B D HD OE A D S D T P F ( TA T E T EL G; ( AHR() O I T S M B C T R A ( A; ( I P F D B T OW  (  T W O C A S() TAH T E T E C G(()  D S T R P T P P B T I U O A HHR A]
[DELETED: H 7()  D S I A R M P THR.() A(  B  ( I,Y T  ( S, A I PS PS A  ); ( B,N R HRD  (  T P D W T HHR A() A(  B HR.() I(N C N : ( () G–UPTOD T R N I A RKAIRMF A N T EIST;  () IC41II  O T N R T;  () IN I R AE   () VN R M F D T EG; ( T;]
[DELETED:  H  ( T,,E S HRM  (  S A V O D P B THR.() EX A O P I T SE A D ( C A I A O A DHR; (  I A F A L  Y A T E O THR; ( N, L T O E YR A W AHR A I S D B T D I ( M, AE A T T O DT A   1 ( P A C S T A HHR A  () CO A D P B T HHRR  () AIPHRM N (  A C A P D SRS:  () SU T HHR A I]
[DELETED: H 9  () SIURS K O R F R O A A T D HHR A  () D,IEE E R   () I.VS()   T A W B A T B I O T CR() A D N N P A R M PY AT  A() ( E50–;  LT ( THR ( D N T T HHR A I ( U T HHR A I S A B I DR A D U  C O T§1(E  ( M:  () OHRM ;  () IH I S S T A IA§1()S D U  A O T S]
[DELETED:  H  ( N A C S T T HHR AHR ( D T P O T HHR AE HR; ( P; ( PHR (  I R H T A T  C 1()); ( D T D T T HHR A ( DHR I S I D T M O C T ( D T D T W T HHR A  ); ( D T D T T HHR A S U I M O C T M T ( B P D T CR O I M ( B; (  U I CS DS S AS A,S ]
[DELETED: H 1 (  M A I A F T I A T() A:N ( A;R  ( W9  D A A I A SHR.() A:N ( T, PE I U CS A B O THR; ( T C I W T HHR A (  A D P B T HHR A ( D O S T T M K OHR; ( AHR ( D O A D U B A D THR; ( D O A M U T E THR–; ( AD O K L O T HHR ( D O T M I T HHR A I SM I]
[DELETED:  H  ( A’D O T DRS O PS  () U;SD   () TIH DRS P F A IHRM  ( HRM HRRS.() A(N I A U S B O T S ( A M S S HHRS  (  B C I O T C W A() H I A R P T A D T P U  A O T S I T1( ( §12; E  ( RAG. T E() H I A R P T A D T P U  A O T S I T1( ( §13; E ]
[DELETED: H 1 ( RAG. T E() ( TAGH T E MY I A F A M T EL G P D O R U T S FES. ( D O D M D T P T E T S O I O P B T O ( IA P T T T E U A UA. C ( YCKP() E()X A P I S B O T SN A ( A,R E  I  E 13; (  T T E A P P I  E    13,§18.() H5.ACVLC.  G O O I L() N T S T F W H T M() “A”§3RE    T I R RFPA.]
[DELETED:  H () “D”I CY M A N CD ER AM SE A TL AE O,,, ( R  (  A T P P O P S I (  A A F T C O U O ANS ( T:  ()  E   () IH L T G T U O TY  ( T:  ()  Y   () I  W W A C B A T E A 3  ( I]
[DELETED: H 1  ( “   ( c f a g s o h o m p r   ( u m a h i t p r a v e a a t p i m t a i a a   ( u  S A B I F E T t A s t e O]